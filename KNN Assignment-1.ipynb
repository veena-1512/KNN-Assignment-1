{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de97331b-e86d-4ff6-91ad-823309f042cc",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d30d13-3af3-4d66-b5a9-e9214d748400",
   "metadata": {},
   "source": [
    "K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "\n",
    "K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
    "\n",
    "K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "\n",
    "K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
    "\n",
    "K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n",
    "\n",
    "It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
    "\n",
    "KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n",
    "\n",
    "Example: Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f58db-4d9a-4a4b-98fe-158d26335411",
   "metadata": {},
   "source": [
    "Q2.How do you choose the value of K in KNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379bf9f0-dcf1-4c05-a95c-ffca3af58a1e",
   "metadata": {},
   "source": [
    "The value of k indicates the number of training samples that are needed to classify the test sample. Coming to your question, the value of k is non-parametric and a general rule of thumb in choosing the value of k is k = sqrt(N)/2, where N stands for the number of samples in your training dataset.\n",
    "\n",
    "There is no particular way to determine the best value for \"K\", so we need to try some values to find the best out of them. The most preferred value for K is 5.\n",
    "\n",
    "A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\n",
    "\n",
    "Large values for K are good, but it may find some difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8eb5c-561d-4feb-8027-ab6cd6a7a16d",
   "metadata": {},
   "source": [
    "Q3.What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643b513-7010-42bb-bac5-d9960314540f",
   "metadata": {},
   "source": [
    "Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors. \n",
    "Classification algorithms are used to forecast or classify the distinct values such as Real or False, Male or Female, Spam or Not Spam, etc.\n",
    "\n",
    "Example: The best example to understand the Classification problem is Email Spam Detection. The model is trained on the basis of millions of emails on different parameters, and whenever it receives a new email, it identifies whether the email is spam or not. If the email is spam, then it is moved to the Spam folder.\n",
    "\n",
    "Knn Regression: Predicts a value by using the mean of the k nearest neighbors.\n",
    "Regression algorithms are used to determine continuous values such as price, income, age, etc.\n",
    "\n",
    "Example:- Regression is a process of finding the correlations between dependent and independent variables. It helps in predicting the continuous variables such as prediction of Market Trends, prediction of House prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5de18e-9553-4185-83b9-05020dcda19d",
   "metadata": {},
   "source": [
    "Q4.How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf6fa0-b06c-47ff-a7b6-5a52df1538f9",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is one of the simplest algorithms used in Machine Learning for regression and classification problems. KNN algorithms use data and classify new data points based on similarity measures (e.g. distance function).\n",
    "\n",
    "he performance of a k-Nearest Neighbors (KNN) algorithm is typically measured using various evaluation metrics, depending on the specific problem you are trying to solve. Commonly used metrics for assessing KNN performance include:\n",
    "\n",
    "Accuracy: This is one of the most straightforward metrics and represents the ratio of correctly predicted instances to the total number of instances in the dataset. It's suitable for balanced datasets but can be misleading in the presence of class imbalance.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Where:\n",
    "\n",
    "TP: True Positives (correctly predicted positive instances)\n",
    "TN: True Negatives (correctly predicted negative instances)\n",
    "FP: False Positives (negative instances predicted as positive)\n",
    "FN: False Negatives (positive instances predicted as negative)\n",
    "Precision: Precision measures the accuracy of positive predictions. It's the ratio of true positives to the total number of positive predictions. It's valuable when minimizing false positives is critical.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the ability of the classifier to find all positive instances. It's the ratio of true positives to the total number of actual positives. It's valuable when minimizing false negatives is important.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1-Score: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. It's particularly useful when the dataset is imbalanced.\n",
    "\n",
    "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "ROC Curve and AUC: Receiver Operating Characteristic (ROC) curves visualize the trade-off between true positive rate (recall) and false positive rate at different classification thresholds. The Area Under the ROC Curve (AUC) quantifies the overall performance of the classifier. AUC values closer to 1 indicate better performance.\n",
    "\n",
    "Confusion Matrix: The confusion matrix provides a comprehensive view of the classifier's performance by showing the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "K-Fold Cross-Validation: To ensure robustness and prevent overfitting, you can use k-fold cross-validation. It involves splitting the dataset into k subsets (folds) and training/testing the model k times, rotating through the folds. This helps estimate the model's performance on unseen data more accurately.\n",
    "\n",
    "Mean Absolute Error (MAE) or Mean Squared Error (MSE) for Regression: If you are using KNN for regression tasks, you can use metrics like MAE or MSE to evaluate the performance of predicted continuous values against the actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448493ee-f6ec-4442-95e4-a7b9df5f88df",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c01b4-285c-4aef-9aaa-0eac1cc15272",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" is a term used to describe the challenges and problems that arise when working with high-dimensional data in machine learning and data analysis, including when using algorithms like k-nearest neighbors (KNN). It refers to several issues that become more pronounced as the dimensionality (the number of features or attributes) of the data increases. Here are some key aspects of the curse of dimensionality in the context of KNN:\n",
    "\n",
    "1. Increased Computational Complexity: As the number of dimensions (features) increases, the number of data points required to maintain the same level of data density also needs to increase exponentially. This means that you'll need a much larger dataset to cover the space adequately, which can lead to increased computational costs in terms of memory and processing time.\n",
    "\n",
    "2. Diminished Discriminatory Power: In high-dimensional spaces, the notion of distance becomes less meaningful. This is because, in high dimensions, most points are roughly equidistant from one another, which makes it difficult to distinguish between nearest neighbors effectively. As a result, KNN may struggle to find meaningful neighbors, leading to decreased accuracy.\n",
    "\n",
    "3. Increased Overfitting Risk: With a large number of dimensions, KNN becomes more susceptible to overfitting, especially if the dataset is relatively small. Finding the nearest neighbors in a high-dimensional space can lead to noise or irrelevant features influencing the classification decision.\n",
    "\n",
    "4. Storage Requirements: As the dimensionality increases, the storage requirements for the distance metric (e.g., Euclidean distance) between data points also increase. This can be a concern when working with large datasets.\n",
    "\n",
    "5. Curse of Sparsity: In high-dimensional spaces, data points tend to be sparse, meaning that there may be large regions of empty space. This sparsity can lead to an uneven distribution of data, making it challenging to find meaningful neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65757178-bcce-495a-a6a7-9434da037392",
   "metadata": {},
   "source": [
    "To mitigate the curse of dimensionality in KNN and other high-dimensional data scenarios, you can consider the following strategies:\n",
    "\n",
    "1. Feature Selection/Dimensionality Reduction: Carefully select relevant features or apply dimensionality reduction techniques like Principal Component Analysis (PCA) to reduce the dimensionality while preserving the most important information.\n",
    "\n",
    "2. Feature Scaling: Normalize or standardize your features to ensure that they contribute equally to distance calculations.\n",
    "\n",
    "3. Domain Knowledge: Use domain knowledge to identify and focus on the most important features for your specific problem, rather than using all available features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede64f6e-127b-4435-842b-978e9a69eaf2",
   "metadata": {},
   "source": [
    "6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af471f8a-cba4-4f19-883d-c536484eef1a",
   "metadata": {},
   "source": [
    "Handling missing values in k-nearest neighbors (KNN) can be a bit tricky because KNN relies on distance metrics to determine the similarity between data points. If you have missing values in your dataset, you'll need to impute or fill in those missing values in a way that doesn't disrupt the distance calculations significantly. Here's a common approach to handling missing values in KNN using Python:\n",
    "\n",
    "1. Impute Missing Values: Choose an appropriate imputation strategy to fill in the missing values. Some common strategies include mean imputation, median imputation, or imputation using the most frequent value. You can use libraries like scikit-learn or pandas for this purpose.\n",
    "\n",
    "2. Scaling: Ensure that your data is scaled appropriately before applying KNN. Since KNN relies on distance measures, it's crucial that all features have the same scale. You can use techniques like Min-Max scaling or standardization (z-score scaling) to achieve this.\n",
    "\n",
    "3. Distance Metric: Be mindful of the choice of distance metric. Some distance metrics (e.g., Euclidean distance) can be sensitive to the presence of missing values. You might consider using alternative distance metrics like the Manhattan distance (L1 norm) or Minkowski distance with a p-value other than 2, which can handle missing values more gracefully.\n",
    "\n",
    "4. Imputation Strategy with Neighbors: An advanced approach is to impute missing values using information from neighboring data points. For each data point with missing values, you can compute the average (or weighted average) of the corresponding feature from its k-nearest neighbors and use that as the imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de050fc-452e-4b44-8893-bce16e48933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on the training data and transform both training and testing data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Standardize the data to ensure all features have the same scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the 'n_neighbors' parameter\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb904d-6185-4fd3-a550-bf9398d0d529",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, \n",
    "and how can these be addressed? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da773b2-247c-4e78-9770-1a7edbbef99f",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple and intuitive machine learning algorithm that can be used for both classification and regression tasks. However, it has its strengths and weaknesses, and understanding these can help you use KNN effectively while addressing its limitations:\n",
    "\n",
    "Strengths of KNN:\n",
    "\n",
    "1. Simplicity: KNN is easy to understand and implement, making it a good choice for beginners in machine learning.\n",
    "\n",
    "2. Non-parametric: KNN is non-parametric, meaning it doesn't make strong assumptions about the underlying data distribution. This makes it versatile and suitable for various types of data.\n",
    "\n",
    "3. Adaptability: KNN can be used for both classification and regression tasks, making it a versatile algorithm.\n",
    "\n",
    "4. No Training Required: KNN doesn't require a training phase since it stores the entire dataset. This can be advantageous when the dataset is small or when it's continuously updated.\n",
    "\n",
    "\n",
    "\n",
    "Weaknesses of KNN:\n",
    "\n",
    "1. Computational Complexity: KNN can be computationally expensive, especially for large datasets and high-dimensional data. Calculating distances between data points can be time-consuming.\n",
    "\n",
    "2. Sensitivity to Noise: KNN is sensitive to noisy data and outliers. Outliers can significantly affect the results by pulling the decision boundary toward them.\n",
    "\n",
    "3. Curse of Dimensionality: In high-dimensional spaces, the concept of distance becomes less meaningful, and all data points can appear equidistant from a query point. This can lead to poor performance.\n",
    "\n",
    "4. Parameter Sensitivity: The choice of the value of 'k' (the number of neighbors to consider) can significantly impact the model's performance. Selecting an appropriate 'k' is often a trial-and-error process.\n",
    "\n",
    "5. Imbalanced Data: KNN can be biased toward the majority class in classification tasks when dealing with imbalanced datasets.\n",
    "\n",
    "\n",
    "Addressing the Weaknesses:\n",
    "\n",
    "1. Dimensionality Reduction: Consider dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection to reduce the curse of dimensionality and improve KNN's performance.\n",
    "\n",
    "2. Data Preprocessing: Address noise and outliers through data preprocessing techniques such as data cleaning, outlier detection, and feature scaling.\n",
    "\n",
    "3. Distance Metric Selection: Choose an appropriate distance metric (e.g., Euclidean, Manhattan, or custom distance functions) based on the characteristics of your data. Experiment with different metrics to find the most suitable one.\n",
    "\n",
    "4. Cross-Validation: Use cross-validation to tune hyperparameters, including 'k', to find the best configuration for your dataset. This helps mitigate the sensitivity to parameter choices.\n",
    "\n",
    "5. Weighted KNN: Assign different weights to neighbors based on their distance from the query point. Closer neighbors can have a higher influence on the prediction, which can help address class imbalances.\n",
    "\n",
    "6. Ensemble Methods: Combine multiple KNN models or use ensemble methods like bagging or boosting to improve robustness and generalization.\n",
    "\n",
    "7. Parallelization: For large datasets, consider parallelization techniques to speed up the computation of distances.\n",
    "\n",
    "8. Local Methods: Explore variations of KNN like \"kernelized\" or \"local\" versions, which can adapt to local data structures more effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fdc3e-3730-46cd-b48a-a40cdab14acc",
   "metadata": {},
   "source": [
    "Q9.What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a155f-0ed2-47ac-870e-8b1f35552850",
   "metadata": {},
   "source": [
    "Euclidean distance captures the same by aggregating the squared difference in each variable.\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Euclidean distance, also known as L2 distance, calculates the straight-line or \"as-the-crow-flies\" distance between two points in Euclidean space (typically 2D or 3D space).\n",
    "It is calculated using the Pythagorean theorem and represents the length of the shortest path between two points.\n",
    "\n",
    "In a 2D space with points (x1, y1) and (x2, y2), the Euclidean distance is given by: root (x1-x2)^2 +(y1-y2)^2.\n",
    "\n",
    "In higher-dimensional spaces, the formula generalizes to include all dimensions.\n",
    "\n",
    "Manhattan Distance:\n",
    "\n",
    "Manhattan distance, also known as L1 distance or taxicab distance, calculates the distance between two points by summing the absolute differences between their coordinates along each dimension.\n",
    "\n",
    "It is named \"Manhattan distance\" because it's like measuring the distance a taxi would travel in a grid-like city where you can only move along streets (i.e., you can't take diagonal shortcuts).\n",
    "\n",
    "In a 2D space with points (x1, y1) and (x2, y2), the Manhattan distance is given by:\n",
    "∣x2−x1∣+∣y2−y1∣\n",
    "\n",
    "In higher-dimensional spaces, you sum the absolute differences for each dimension.\n",
    "The Manhattan distance is a distance metric between two points. It's the sum of the absolute differences between these points' coordinates. It's also known by other names:\n",
    "\n",
    "The taxicab distance;\n",
    "The city block distance; and\n",
    "The snake distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887ed42-bba8-408c-9b79-c338185afb8b",
   "metadata": {},
   "source": [
    "10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655d989-0704-4601-a3ff-0acc3ca362ab",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in K-Nearest Neighbors (KNN) and many other machine learning algorithms. It is the process of transforming the features (variables) of your dataset to have the same scale or range. In the context of KNN, feature scaling is important for the following reasons:\n",
    "\n",
    "1. Distance Metric: KNN relies on measuring distances between data points to determine their similarity. If the features are on different scales, those with larger magnitudes can dominate the distance calculations. Features with larger scales may contribute disproportionately to the distance, leading to biased results. Feature scaling helps in mitigating this issue.\n",
    "\n",
    "2. Dimensional Equivalence: In KNN, each feature contributes to the distance calculation equally. If the features are on different scales, the algorithm may give undue importance to certain dimensions. Scaling ensures that all features are treated equally in terms of their impact on distance computations.\n",
    "\n",
    "3. Convergence: Scaling can help the KNN algorithm converge faster and produce more accurate results. It can prevent the algorithm from taking longer paths when calculating distances due to differences in feature scales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdce73-c039-46f7-af9c-d86ef6616dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
